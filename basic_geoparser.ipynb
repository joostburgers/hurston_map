{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a623198-174f-44bc-aa35-0c3de16393d3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Their Eyes Were Watching God (Map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545d29cb-efdb-47af-8194-1c655d3c9d35",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This project maps all locations mentioned in *Their Eyes Were Watching God* using Machine Learning tools. The script processes the text, extracts toponyms (place names) using a geoparser library, and assigns coordinates to each location.\n",
    "\n",
    "The locations are visualized on a map, where the size of each circle corresponds to the frequency of mentions. Clicking a circle reveals the underlying text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b51b58-f44e-4136-90fb-7e07fd39b1ca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 1 Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e93b30-85e5-42fa-a6e7-2a0032c655a7",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell",
     "remove-cell"
    ]
   },
   "source": [
    "### Step-by-Step Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9f5f2a2-2f92-4232-bb90-e66bb23679ae",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705ec040-c894-4f91-8a4a-10eb982f1adc",
   "metadata": {},
   "source": [
    "### Import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03226c8b-c440-4d6e-bc0c-d92c6d46cd7a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "file_path = \"data/hurston_their_eyes_were_watching_god.txt\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Create a DataFrame with the entire text in one cell\n",
    "df = pd.DataFrame([[text]], columns=[\"Text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d76f05-7766-423b-a48c-ff1c3a84643b",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00866dad-11dd-4503-89da-618a7bf8cbbf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 2 Clean DataFrame for Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd300887-ac66-40d0-89e3-e5cd9e70f019",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "To prepare our data for analysis we will: \n",
    "\n",
    "- Split it into sentences\n",
    "- Clean the individual sentences\n",
    "- Drop unnecessary data\n",
    "\n",
    "For the code below you will have to replace `df_dickens` with the name of your dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ddb570-bbea-4e67-920a-b2416528b8cd",
   "metadata": {},
   "source": [
    "#### Step 1: Tokenize Text into Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4625bbb-dc5b-4876-9e5d-034592781008",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\burgerjx/nltk_data'\n    - 'C:\\\\Users\\\\burgerjx\\\\AppData\\\\Local\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\burgerjx\\\\AppData\\\\Local\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\burgerjx\\\\AppData\\\\Local\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\burgerjx\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Explodes the DataFrame so that each row corresponds to a single sentence\u001b[39;00m\n\u001b[0;32m      2\u001b[0m df_sentences \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39massign(\n\u001b[1;32m----> 3\u001b[0m     sentences\u001b[38;5;241m=\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(nltk\u001b[38;5;241m.\u001b[39msent_tokenize)\n\u001b[0;32m      4\u001b[0m )\u001b[38;5;241m.\u001b[39mexplode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentences\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4919\u001b[0m         func,\n\u001b[0;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1509\u001b[0m )\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m _open(resource_url)\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, path \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\burgerjx/nltk_data'\n    - 'C:\\\\Users\\\\burgerjx\\\\AppData\\\\Local\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\burgerjx\\\\AppData\\\\Local\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\burgerjx\\\\AppData\\\\Local\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\burgerjx\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Explodes the DataFrame so that each row corresponds to a single sentence\n",
    "df_sentences = df.assign(\n",
    "    sentences=df['Text'].apply(nltk.sent_tokenize)\n",
    ").explode('sentences')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fe3929-f97f-45f6-9c50-cb2109f0067d",
   "metadata": {},
   "source": [
    "#### Step 2: Drop remaining column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ac8ba14-0b48-4751-bddf-b3d5ea93a7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences = df_sentences.drop(columns='Text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e2f7bd-0c9d-4a63-b967-f0ebee9eb176",
   "metadata": {},
   "source": [
    "### Step 3: Define a Cleaning Function for Sentences\n",
    "\n",
    "The following function goes through and cleans all the sentences. This is a common procedure in this type of analysis. All of the text needs to be in the some format in order for it to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad18ac15-75ad-4dca-a43c-84ddd832e49d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    # 1. Remove text inside square brackets\n",
    "    sentence = re.sub(r'\\[.*?\\]', '', sentence)\n",
    "    # 2. Remove unwanted punctuation but retain sentence-ending punctuation\n",
    "    sentence = re.sub(r'[^\\w\\s,.!?\\'\"‘’“”`]', '', sentence)\n",
    "    # 3. Remove newline and carriage return characters, and underscores\n",
    "    sentence = sentence.replace('\\n', ' ').replace('\\r', ' ').replace('_', '')\n",
    "    # 4. Return an empty string for all-uppercase sentences (likely headers or TOC entries)\n",
    "    return '' if sentence.isupper() else sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ea310d-6b6e-4a26-a8e6-8a755d47ae26",
   "metadata": {},
   "source": [
    "### Step 4: Apply Cleaning and Remove Empty Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b711517-7800-400c-a71f-0638f405f07b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply the cleaning function, then filter out any sentences that are empty strings\n",
    "df_sentences['cleaned_sentences'] = df_sentences['sentences'].apply(clean_sentence)\n",
    "df_sentences = df_sentences[df_sentences['sentences'] != '']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd44120-c579-4e11-bb24-ab9123d2f249",
   "metadata": {},
   "source": [
    "### Step 5: Reset Index for the Cleaned DataFrame\n",
    "\n",
    "This is a necessary step for Pandas to keep track of the row numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48e9b05c-6c30-4d7e-b1f4-5f185f3dbd27",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_sentences = df_sentences.reset_index(drop=True)\n",
    "df_sentences = df_sentences.drop(columns='sentences')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d9a9e3-5175-46ac-8a80-acaf6f4961b0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 2 Geoparsing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9911c805-4185-4d35-aa32-75c504d67e17",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Overview\n",
    "\n",
    "Geoparsing identifies place names in text and assigns them geographic coordinates. Some limitations:\n",
    "1. It can only detect explicitly named locations.\n",
    "2. It makes an educated guess when multiple locations share the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5771fac4-63cf-48f4-a8a4-75f12db32639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geoparser import Geoparser\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbde5075-2eeb-4117-9a9a-53e9bf9bf4dd",
   "metadata": {},
   "source": [
    "Because there are some compatibility issues with the `geoparser` package, there are pesky warnings that pop-up. These do not affect the output, but they are annoying. The line below filters these out of the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db235c6c-b309-4a77-a655-09b8154eac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress all FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978acb25-34b1-4903-b3e1-ac5ab017245f",
   "metadata": {},
   "source": [
    "### Load Geoparser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa3c57f-4c2b-444c-9b55-8ed504cb54ea",
   "metadata": {},
   "source": [
    "To use Geoparser, instantiate an object of the Geoparser class with optional specifications for the spaCy model, transformer model, and gazetteer. By default, the library uses an accuracy-optimised configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7911b1e0-e961-4c34-953c-9706a47b2443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c55c5860-4964-4ccd-b224-62401445a609",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Geoparser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m geo \u001b[38;5;241m=\u001b[39m Geoparser(spacy_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_trf\u001b[39m\u001b[38;5;124m'\u001b[39m, transformer_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdguzh/geo-all-distilroberta-v1\u001b[39m\u001b[38;5;124m'\u001b[39m, gazetteer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeonames\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Geoparser' is not defined"
     ]
    }
   ],
   "source": [
    "geo = Geoparser(spacy_model='en_core_web_trf', transformer_model='dguzh/geo-all-distilroberta-v1', gazetteer='geonames')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b05bd9-bbd9-47b9-b6b4-7da886cc9b8a",
   "metadata": {},
   "source": [
    "Load in the `geoparse_column` function to simplify the toponym recognition process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868ef385-d77d-406d-8707-18959d7a458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geoparse_column(df):\n",
    "    sentences = df['cleaned_sentences'].tolist()  # Convert column to list\n",
    "    docs = geo.parse(sentences, feature_filter=['A', 'P'])  # Run geo.parse on the entire list\n",
    "\n",
    "    # Initialize lists to store the extracted fields\n",
    "    places, latitudes, longitudes, feature_names = [], [], [], []\n",
    "\n",
    "    # Iterate through the results and extract toponyms and their locations\n",
    "    for doc in docs:\n",
    "        doc_places = []\n",
    "        doc_latitudes = []\n",
    "        doc_longitudes = []\n",
    "        doc_feature_names = []\n",
    "\n",
    "        for toponym in doc.toponyms:\n",
    "            if toponym.location:\n",
    "                doc_places.append(toponym.location.get('name'))\n",
    "                doc_latitudes.append(toponym.location.get('latitude'))\n",
    "                doc_longitudes.append(toponym.location.get('longitude'))\n",
    "                doc_feature_names.append(toponym.location.get('feature_name'))\n",
    "            else:\n",
    "                doc_places.append(None)\n",
    "                doc_latitudes.append(None)\n",
    "                doc_longitudes.append(None)\n",
    "                doc_feature_names.append(None)\n",
    "\n",
    "        # Append the extracted data for the document\n",
    "        places.append(doc_places)\n",
    "        latitudes.append(doc_latitudes)\n",
    "        longitudes.append(doc_longitudes)\n",
    "        feature_names.append(doc_feature_names)\n",
    "\n",
    "    # Assign the extracted data to the DataFrame as new columns\n",
    "    df['place'] = places\n",
    "    df['latitude'] = latitudes\n",
    "    df['longitude'] = longitudes\n",
    "    df['feature_name'] = feature_names\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375790a9-d5cb-4184-bd31-b656766caff2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "geoparse_column(df_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23545418-afd0-45ee-9c46-8386f2055305",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Export Pickle 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e67b1cc-0d10-406f-9348-6bd879ebd2cd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "As the geoparsing process takes a long time, you should store it right after the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a89aee-b5d9-4b46-a3ed-3c07a255279f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_sentences.to_pickle('df_sentences_places.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ac9ea1-5eaf-4134-97f2-8e37c4b5a1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences= pd.read_pickle('basic_geoparser/df_sentences_places.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d0a2a7-d001-493a-a7d5-89c75d30f66d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Clean up the resulting dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047f0fec-3c82-4598-a95f-9747cddad4d7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "As with the previous instance of toponym resolution, there will be some rows that do not contain relevant information. This will slow down the sentiment analysis. \n",
    "1. Eliminate empty results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1e6728-e3e5-4fa9-ab95-668334df78d4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_places = df_sentences[df_sentences['place'].apply(lambda x: isinstance(x, list) and x != [None] and len(x) > 0)].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad92253-d754-4d6f-907f-9bb33b494184",
   "metadata": {},
   "source": [
    "#### Unnest Places\n",
    "\n",
    "Since sentences can contain multiple locations, occassionally there will be multiple locations per row. These have to be `unnested` that is turned into individual rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d1bfac-25dc-4857-b861-755fe0e67279",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_places = df_places.explode(['place', 'latitude', 'longitude', 'feature_name']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99107912-c139-4a02-ae42-bbff495b1c81",
   "metadata": {},
   "source": [
    "### Aggregate place\n",
    "\n",
    "Once the places are unnested they can be aggretated and each instance of a place can be counted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc75fa3-aa23-457f-9c22-30cc7759b2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consolidated = df_places.groupby('place').agg(\n",
    "    sentence_count=('cleaned_sentences', 'count'),\n",
    "    sentences=('cleaned_sentences', list),\n",
    "    latitude=('latitude', 'first'),\n",
    "    longitude=('longitude', 'first'),\n",
    "    feature_name=('feature_name', 'first')\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccdba55-ed07-4c5f-b50b-6478a06f63de",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Generally, the data will be very left skewed. You might want to filter out some of the lower values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecef527d-3d4d-48ac-916b-3e8da942f104",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_dickens = df_dickens[df_dickens.location_count>50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa92368a-55db-45a1-8041-2e8e380dd4f5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Bucket Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b12403-bd56-4358-a974-5c5ae4c917f0",
   "metadata": {},
   "source": [
    "When mapping quantitative data you want to break it down into buckets so that it is easier to differentiate small from large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5bf278-7259-411b-af49-f2795836ff52",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mapclassify as mc #you may get an error. If so install mapclassify with pip install mapclassify\n",
    "\n",
    "jenks_breaks = mc.NaturalBreaks(y=df_consolidated['sentence_count'], k=5)\n",
    "df_consolidated.loc[:,'location_count_bucket'] = jenks_breaks.find_bin(df_consolidated['sentence_count'])+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88db5d13-7875-4399-a34f-e7171d3bb790",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Export Pickle 3\n",
    "\n",
    "Exporting the file to save it for later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a5a5b2-b26c-4a8b-afae-a256aef28174",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_consolidated.to_pickle('basic_geoparser/df_consolidated_buckets.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a09fc2-1bc4-45cc-b9cf-010bf9de1bb6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa370e7-11ea-4724-8740-fb28b1689715",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Map your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e3a5bc-326f-421c-9076-9ddecb303e4e",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "With all this data we can create a custom map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4807f6a-0f82-4fdc-9157-9f1c0a590348",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert list of sentences into a single string for hover text\n",
    "\n",
    "import dash\n",
    "from dash import dcc, html\n",
    "from dash.dependencies import Input, Output\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "jenks_labels = {\n",
    "    1: f\"{min(df_consolidated['sentence_count'])}\",\n",
    "    2: f\"{int(jenks_breaks.bins[1])}\",\n",
    "    3: f\"{int(jenks_breaks.bins[1])} - {int(jenks_breaks.bins[2])}\",\n",
    "    4: f\"{int(jenks_breaks.bins[2])} - {int(jenks_breaks.bins[3])}\",\n",
    "    5: f\"{int(jenks_breaks.bins[3])} - {max(df_consolidated['sentence_count'])}\"\n",
    "}\n",
    "\n",
    "# Map the bucket numbers to labels\n",
    "df_consolidated[\"bucket_label\"] = df_consolidated[\"location_count_bucket\"].map(jenks_labels)\n",
    "\n",
    "# Create the scatter map\n",
    "fig = px.scatter_mapbox(\n",
    "    df_plot,\n",
    "    lat=\"latitude\",\n",
    "    lon=\"longitude\",\n",
    "    size=\"location_count_bucket\",  # Bubble sizes scale correctly\n",
    "    color=\"bucket_label\",  # Legend shows Jenks bucket ranges\n",
    "    \n",
    "    hover_name=\"place\",\n",
    "    size_max=20,\n",
    "    category_orders={\"bucket_label\": list(jenks_labels.values())},  # Keep correct legend order\n",
    "    center={\"lat\": 27, \"lon\": -81},\n",
    "    zoom=3\n",
    ")\n",
    "\n",
    "# Ensure legend bubbles are properly sized\n",
    "fig.update_traces(marker=dict(sizemode=\"area\"), selector=dict(mode=\"markers\"))\n",
    "\n",
    "# Update layout to improve legend readability\n",
    "fig.update_layout(\n",
    "    mapbox_style=\"carto-positron\",\n",
    "    margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0},\n",
    "    legend_title_text=\"Location Count\"\n",
    ")\n",
    "\n",
    "# Create Dash App\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    dcc.Graph(id=\"map\", figure=fig),  # Display the map\n",
    "    html.Div(id=\"output-container\", style={'padding': '20px', 'font-size': '16px'})  # Placeholder for dialog box\n",
    "])\n",
    "\n",
    "# Callback to update dialog on click\n",
    "@app.callback(\n",
    "    Output(\"output-container\", \"children\"),\n",
    "    Input(\"map\", \"clickData\")\n",
    ")\n",
    "def display_dialog(clickData):\n",
    "    if clickData:\n",
    "        clicked_place = clickData[\"points\"][0][\"hovertext\"]  # Get the place name\n",
    "        sentences = df_consolidated[df_consolidated[\"place\"] == clicked_place][\"sentences\"].values\n",
    "        if len(sentences) > 0:\n",
    "            sentences_list = \"<br>\".join(sentences[0])  # Format sentences for display\n",
    "            return html.Div([\n",
    "                html.H3(f\"Sentences about {clicked_place}:\"),\n",
    "                html.P(sentences_list, style={'white-space': 'pre-line'})\n",
    "            ])\n",
    "    return \"Click a location to see related sentences.\"\n",
    "\n",
    "# Run the app\n",
    "if __name__ == \"__main__\":\n",
    "    app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea273a46-697a-4829-b795-f81f36b179a8",
   "metadata": {},
   "source": [
    "Happy mapping!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce5995c-e3a7-427b-826b-d80cc1984d20",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
